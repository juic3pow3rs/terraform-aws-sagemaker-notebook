{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Should you question an invoice sent by a supplier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For updates on the way Sagemaker or AWS behave compared to the notebook code, please refer to https://livebook.manning.com/#!/book/machine-learning-for-business/chapter-5/v-5/137"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Load and examine the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the code in the notebook cell, change the name of the data_bucket from 'machliba' to the data_bucket holding your data and click into the cell and press Ctrl+Enter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bucket = 'ie-mlforbusiness-01' \n",
    "subfolder = 'ch05' \n",
    "dataset = 'activities.csv' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import s3fs\n",
    "import sagemaker\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import csv\n",
    "from time import sleep\n",
    "\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "s3 = s3fs.S3FileSystem(anon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f's3://{data_bucket}/{subfolder}/{dataset}')\n",
    "display(df[5:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count entries with and without errors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Statistics on individual subject areas, resource types and type of activity\n",
    "\n",
    "print(f'Number of rows in dataset: {df.shape[0]}')\n",
    "print()\n",
    "print('Matter types:')\n",
    "print(df['Matter Type'].value_counts())\n",
    "print()\n",
    "print('Resources:')\n",
    "print(df['Resource'].value_counts())\n",
    "print()\n",
    "print('Activities:')\n",
    "print(df['Activity'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Get the data into the right shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Transfomation of data set --> One-Hot Encoding\n",
    "\n",
    "encoded_df = pd.get_dummies(df, columns=['Matter Type','Resource','Activity']) \n",
    "encoded_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Create training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create train- and validation data set\n",
    "\n",
    "train_df, val_df, _, _ = train_test_split(encoded_df, encoded_df['Error'], test_size=0.2, random_state=0)\n",
    "train_df_no_result = train_df.drop(['Error','Firm Name'], axis=1)\n",
    "val_df_no_result = val_df.drop(['Error','Firm Name'], axis=1)\n",
    "print(f'{train_df.shape[0]} rows in training data')\n",
    "print(f'{val_df.shape[0]} rows in validation data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Train the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import von RandomCutForest\n",
    "\n",
    "from sagemaker import RandomCutForest\n",
    "\n",
    "session = sagemaker.Session()\n",
    "\n",
    "rcf = RandomCutForest(role=role,\n",
    "                      train_instance_count=1,\n",
    "                      train_instance_type='ml.m4.xlarge', # set instance type\n",
    "                      data_location=f's3://{data_bucket}/{subfolder}/',#set path to locate data set\n",
    "                      output_path=f's3://{data_bucket}/{subfolder}/output', #set path for model output\n",
    "                      num_samples_per_tree=100, # Number of samples per tree; recommended because it povides good middle\n",
    "                                                  #ground between speed and size \n",
    "                      num_trees=50) # Number of trees, set at error rate (2% = 1/50)\n",
    "\n",
    "# automatically upload the training data to S3 and run the training job\n",
    "rcf.fit(rcf.record_set(train_df_no_result.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Host the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hosting model - create endpoint\n",
    "\n",
    "endpoint_name = 'suspicious-lines'\n",
    "try:\n",
    "    session.delete_endpoint(endpoint_name)\n",
    "    sess.delete_endpoint_config(endpoint_name)\n",
    "    print('Warning: Existing endpoint deleted to make way for your new endpoint.')\n",
    "    sleep(30)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hosting model - set resources\n",
    "\n",
    "rcf_endpoint = rcf.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.t2.medium', \n",
    "    endpoint_name=endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting data to workable format\n",
    "\n",
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "from sagemaker.deserializers import JSONDeserializer   \n",
    "from sagemaker.serializers import CSVSerializer        \n",
    "\n",
    "#rcf_endpoint.content_type = 'text/csv'\n",
    "#rcf_endpoint.__setattr__(rcf_endpoint.content_type, \"text/csv\")\n",
    "rcf_endpoint.serializer = CSVSerializer()\n",
    "#rcf_endpoint.accept = 'application/json'\n",
    "rcf_endpoint.deserializer = JSONDeserializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#calculat anomaly scores\n",
    "results = rcf_endpoint.predict(val_df_no_result.values)\n",
    "#Scores in neuem DataFrame festhalten\n",
    "scores_df = pd.DataFrame(results['scores'])\n",
    "\n",
    "\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "\n",
    "#adding scores to our validation data set\n",
    "results_df = pd.concat([val_df, scores_df], axis=1)\n",
    "\n",
    "#Output the number of errors in val_df: 20791 without error, 402 with error\n",
    "results_df['Error'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determination of the median of all scores for data points that are in error (Threshold)\n",
    "score_cutoff = results_df[results_df['Error'] == True]['score'].median()\n",
    "print(f'Score cutoff: {score_cutoff}')\n",
    "\n",
    "#new dataframe for scores that are above median\n",
    "results_above_cutoff = results_df[results_df['score'] > score_cutoff]\n",
    "\n",
    "#output threshold\n",
    "#Number of data points above the threshold that actually represent errors\n",
    "#Number of data points above the threshold that have been flagged as an anomaly but are not an anomaly.\n",
    "results_above_cutoff['Error'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sets values in the prediction column to True where score > treshold\n",
    "results_df['Prediction'] = results_df['score'] > score_cutoff\n",
    "\n",
    "#results_df.head()\n",
    "\n",
    "results_df.loc[results_df['score'] > score_cutoff]\n",
    "#results_df.loc[results_df['score'] < score_cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "\n",
    "data = {'y_Actual':    results_df['Error'],\n",
    "        'y_Predicted': results_df['Prediction']\n",
    "        }\n",
    "\n",
    "df = pd.DataFrame(data, columns=['y_Actual','y_Predicted'])\n",
    "confusion_matrix = pd.crosstab(df['y_Actual'], df['y_Predicted'], rownames=['Actual'], colnames=['Predicted'])\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sn.heatmap(confusion_matrix, annot=True, linewidths=0.5, fmt = 'd')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate precision, recall, f1\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "score = precision_recall_fscore_support(results_df['Error'],results_df['Prediction'], average='binary')\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determination of the median of all scores for data points that are in error (Threshold)\n",
    "\n",
    "score_cutoff_new = \n",
    "print(f'Score cutoff: {score_cutoff_new}')\n",
    "\n",
    "#new dataframe for scores that are above median\n",
    "results_above_cutoff = results_df[results_df['score'] > score_cutoff_new]\n",
    "\n",
    "#output threshold\n",
    "#Number of data points above the threshold that actually represent errors\n",
    "#Number of data points above the threshold that have been flagged as an anomaly but are not an anomaly.\n",
    "results_above_cutoff['Error'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sets values in the prediction column to True where score > treshold\n",
    "results_df['Prediction'] = results_df['score'] > score_cutoff_new\n",
    "\n",
    "#results_df.head()\n",
    "\n",
    "results_df.loc[results_df['score'] > score_cutoff_new]\n",
    "#results_df.loc[results_df['score'] < score_cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "\n",
    "data = {'y_Actual':    results_df['Error'],\n",
    "        'y_Predicted': results_df['Prediction']\n",
    "        }\n",
    "\n",
    "df = pd.DataFrame(data, columns=['y_Actual','y_Predicted'])\n",
    "confusion_matrix = pd.crosstab(df['y_Actual'], df['y_Predicted'], rownames=['Actual'], colnames=['Predicted'])\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sn.heatmap(confusion_matrix, annot=True, linewidths=0.5, fmt = 'd')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate precision, recall, f1\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "score = precision_recall_fscore_support(results_df['Error'],results_df['Prediction'], average='binary')\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the Endpoint (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment out this cell if you want the endpoint to exist after \"run all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.delete_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
