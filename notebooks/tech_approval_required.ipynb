{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting whether an order should be sent to a technical approver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For updates on the way Sagemaker or AWS behave compared to the notebook code, please refer to https://livebook.manning.com/#!/book/machine-learning-for-business/chapter-2/v-5/67"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Load and examine the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hinterlegen der Bucket credentials\n",
    "\n",
    "data_bucket = 'ie-mlforbusiness-01'\n",
    "subfolder = 'ch02'\n",
    "dataset = 'orders_with_predicted_value.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importieren von Bibliotheken\n",
    "\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "import s3fs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Erstellen einer Rolle, um Sagemaker Ressourcen zu nutzen\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "#Herstellen einer Verbindung zu S3\n",
    "s3 = s3fs.S3FileSystem(anon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Einlesen des datensatzes aus S3 in einen Pnadas DataFrame\n",
    "df = pd.___(f's3://{data_bucket}/{subfolder}/{dataset}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Identifikation von Bestellungen mit und ohne Approval\n",
    "\n",
    "print(f'Number of rows in dataset: {df.shape[0]}')\n",
    "print(df[df.columns[0]].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Get the data into the right shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding der Daten mittels One-Hot Encoding\n",
    "\n",
    "encoded_data = pd.get_dummies(df)\n",
    "encoded_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bestimmung der Korrelation mittels Features und Zielvariable\n",
    "#Anzeigen der Features mit einer Korrelation > 10%\n",
    "\n",
    "corrs = encoded_data.corr()['tech_approval_required'].abs()\n",
    "columns = corrs[corrs > .1].index\n",
    "corrs = corrs.filter(columns)\n",
    "corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Weiterhin nutzen wir nur die Features, die einen Korrelation > 10 % aufweisen\n",
    "\n",
    "encoded_data = encoded_data[columns]\n",
    "encoded_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Create training, validation and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Erstellen von Trainings-, Test- und Validierungsdatensatz\n",
    "\n",
    "#Training 70%, Val 20%, Test 10%\n",
    "train_df, val_and_test_data = train_test_split(encoded_data, test_size=0.3, random_state=0)\n",
    "val_df, test_df = train_test_split(val_and_test_data, test_size=0.333, random_state=0)\n",
    "\n",
    "#Umwandeln der DataFrames in CSV und Speicherung in S3\n",
    "train_data = train_df.to_csv(None, header=False, index=False).encode()\n",
    "val_data = val_df.to_csv(None, header=False, index=False).encode()\n",
    "test_data = test_df.to_csv(None, header=True, index=False).encode()\n",
    "\n",
    "\n",
    "with s3.open(f'{data_bucket}/{subfolder}/processed/train.csv', 'wb') as f:\n",
    "    f.write(train_data)\n",
    "\n",
    "with s3.open(f'{data_bucket}/{subfolder}/processed/val.csv', 'wb') as f:\n",
    "    f.write(val_data) \n",
    "    \n",
    "with s3.open(f'{data_bucket}/{subfolder}/processed/test.csv', 'wb') as f:\n",
    "    f.write(test_data) \n",
    "    \n",
    "#Load data in sagemaker\n",
    "train_input = sagemaker.TrainingInput(s3_data=f's3://{data_bucket}/{subfolder}/processed/train.csv', content_type='csv') # s3_input\n",
    "val_input = sagemaker.TrainingInput(s3_data=f's3://{data_bucket}/{subfolder}/processed/val.csv', content_type='csv')    # s3_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store SageMaker Session\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "#Definition eines Containers, in dem AWS das Modell speichern kann\n",
    "container = sagemaker.amazon.amazon_estimator.get_image_uri(\n",
    "                boto3.Session().region_name,\n",
    "                'xgboost',\n",
    "                'latest')\n",
    "\n",
    "#Erstellen des Modells\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "                container,\n",
    "                role,\n",
    "                train_instance_count=1, \n",
    "                train_instance_type='ml.m4.xlarge',#Server-Typ = Umfang vorhandener Ressourcen\n",
    "                output_path=f's3://{data_bucket}/{subfolder}/output', #Speichert Output in S3\n",
    "                sagemaker_session=sess)\n",
    "\n",
    "#Definition von Hyperparametern für das Modell\n",
    "estimator.set_hyperparameters(\n",
    "                max_depth=5, #max Tiefe eines Baumes, je tiefer desto komplexer (Overfitting)\n",
    "                subsample=0.7, # 30% der Daten bleiben für das einzelne Model ungenutzt\n",
    "                objective='binary:logistic', #logistische Regression für binäre Klassifikation\n",
    "                eval_metric = 'auc', # HP tunen sodass bester AUC-Wert erreicht wird\n",
    "                num_round=100, # Anzahl der Trainingsrunden\n",
    "                early_stopping_rounds=10) # Anzahl der Trainingsrunden nach denen das Training ohne Verbesserung beendet wird\n",
    "\n",
    "#Trainieren/Fitten des Modells\n",
    "estimator.fit({'train': train_input, 'validation': val_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Host the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Erstellen eines Endpoints über den später auf das Modell zugegriffen werden kann\n",
    "\n",
    "endpoint_name = 'order-approval'\n",
    "try:\n",
    "    sess.delete_endpoint(endpoint_name)\n",
    "    sess.delete_endpoint_config(endpoint_name)\n",
    "    print('Warning: Existing endpoint deleted to make way for your new endpoint.')\n",
    "    sleep(30)\n",
    "except:\n",
    "    pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bereitstellen eines Servers, um das Modell zu hosten\n",
    "predictor = estimator.deploy(initial_instance_count=1,\n",
    "               instance_type='ml.t2.medium',\n",
    "               endpoint_name=endpoint_name\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ermöglicht predictor CSV Formate einfacher zu handhaben\n",
    "\n",
    "from sagemaker.predictor import csv_serializer, json_serializer\n",
    "from sagemaker.deserializers import JSONDeserializer   \n",
    "from sagemaker.serializers import CSVSerializer        \n",
    "#predictor.content_type = 'text/csv'\n",
    "predictor.__setattr__(predictor.content_type, \"text/csv\")\n",
    "predictor.serializer = CSVSerializer()      #csv_serializer\n",
    "predictor.deserializer = JSONDeserializer() #None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fkt, die alle Zeilen des Testdatensatzes an den Prädiktor weitergibt und eine Vorhersage zurückgibt.\n",
    "\n",
    "def get_prediction(row):\n",
    "    prediction = round(float(predictor.predict(row[1:]))) #.decode('UTF-8')\n",
    "    return prediction\n",
    "\n",
    "#Prädiktion der ungesehenen Testdaten\n",
    "with s3.open(f'{data_bucket}/{subfolder}/processed/test.csv') as f:\n",
    "    test_data = pd.read_csv(f)\n",
    "    \n",
    "#Anwenden von get_prediction auf unsere Testdaten\n",
    "cols = list(test_data.columns)\n",
    "test_data['prediction'] = test_data.apply(get_prediction, axis=1)\n",
    "\n",
    "#Erstellen einer neuen Tabelle mit den Prädiktionen\n",
    "test_data = test_data[['prediction'] + cols]\n",
    "test_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Welche Prädiktionen weichen von der Grundwahrheit ab?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Erstellen einer Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Berechnen der Genauigkeit (Accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bestimmung des F1-Scores des Modells\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the Endpoint (optional)\n",
    "Comment out this cell to remove the endpoint if you want the endpoint to exist after \"run all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Löschen des Endpoints\n",
    "sess.delete_endpoint(endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
