{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting future energy usage from multiple dependent time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For updates on the way Sagemaker or AWS behave compared to the notebook code, please refer to https://livebook.manning.com/#!/book/machine-learning-for-business/chapter-6/v-5/102"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Load and examine the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bucket = 'martin-mlforbusiness-eu'\n",
    "subfolder = 'ch06'\n",
    "dataset = '06_meter_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install boto3==1.26.34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from dateutil.parser import parse\n",
    "import json\n",
    "from random import shuffle\n",
    "import random\n",
    "import datetime\n",
    "import os\n",
    "                            \n",
    "import boto3\n",
    "import s3fs\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "s3 = s3fs.S3FileSystem(anon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Site_1</th>\n",
       "      <th>Site_2</th>\n",
       "      <th>Site_3</th>\n",
       "      <th>Site_4</th>\n",
       "      <th>Site_5</th>\n",
       "      <th>Site_6</th>\n",
       "      <th>Site_7</th>\n",
       "      <th>Site_8</th>\n",
       "      <th>Site_9</th>\n",
       "      <th>Site_10</th>\n",
       "      <th>...</th>\n",
       "      <th>Site_39</th>\n",
       "      <th>Site_40</th>\n",
       "      <th>Site_41</th>\n",
       "      <th>Site_42</th>\n",
       "      <th>Site_43</th>\n",
       "      <th>Site_44</th>\n",
       "      <th>Site_45</th>\n",
       "      <th>Site_47</th>\n",
       "      <th>Site_46</th>\n",
       "      <th>Site_48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-01 00:00:00</th>\n",
       "      <td>13.30</td>\n",
       "      <td>13.3</td>\n",
       "      <td>11.68</td>\n",
       "      <td>13.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.9</td>\n",
       "      <td>11.4</td>\n",
       "      <td>21.8</td>\n",
       "      <td>9.7</td>\n",
       "      <td>11.9</td>\n",
       "      <td>...</td>\n",
       "      <td>22.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.30</td>\n",
       "      <td>36.74</td>\n",
       "      <td>48.48</td>\n",
       "      <td>68.70</td>\n",
       "      <td>32.13</td>\n",
       "      <td>19.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-01 00:30:00</th>\n",
       "      <td>11.75</td>\n",
       "      <td>11.9</td>\n",
       "      <td>12.63</td>\n",
       "      <td>13.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>122.1</td>\n",
       "      <td>11.3</td>\n",
       "      <td>17.7</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.4</td>\n",
       "      <td>...</td>\n",
       "      <td>21.68</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>22.14</td>\n",
       "      <td>36.44</td>\n",
       "      <td>50.18</td>\n",
       "      <td>69.09</td>\n",
       "      <td>29.02</td>\n",
       "      <td>19.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-01 01:00:00</th>\n",
       "      <td>12.58</td>\n",
       "      <td>11.4</td>\n",
       "      <td>11.86</td>\n",
       "      <td>13.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>110.3</td>\n",
       "      <td>10.9</td>\n",
       "      <td>17.5</td>\n",
       "      <td>9.1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>21.88</td>\n",
       "      <td>35.93</td>\n",
       "      <td>49.44</td>\n",
       "      <td>67.52</td>\n",
       "      <td>26.65</td>\n",
       "      <td>20.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-01 01:30:00</th>\n",
       "      <td>12.50</td>\n",
       "      <td>10.8</td>\n",
       "      <td>11.53</td>\n",
       "      <td>11.83</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.6</td>\n",
       "      <td>11.2</td>\n",
       "      <td>16.5</td>\n",
       "      <td>12.4</td>\n",
       "      <td>11.5</td>\n",
       "      <td>...</td>\n",
       "      <td>21.28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>22.69</td>\n",
       "      <td>45.25</td>\n",
       "      <td>49.57</td>\n",
       "      <td>68.48</td>\n",
       "      <td>25.28</td>\n",
       "      <td>19.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-01 02:00:00</th>\n",
       "      <td>12.98</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11.57</td>\n",
       "      <td>12.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>91.4</td>\n",
       "      <td>10.9</td>\n",
       "      <td>16.3</td>\n",
       "      <td>12.4</td>\n",
       "      <td>11.3</td>\n",
       "      <td>...</td>\n",
       "      <td>40.48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>22.75</td>\n",
       "      <td>62.94</td>\n",
       "      <td>48.58</td>\n",
       "      <td>75.30</td>\n",
       "      <td>23.65</td>\n",
       "      <td>19.18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Site_1  Site_2  Site_3  Site_4  Site_5  Site_6  Site_7  \\\n",
       "2017-11-01 00:00:00   13.30    13.3   11.68   13.02     0.0   102.9    11.4   \n",
       "2017-11-01 00:30:00   11.75    11.9   12.63   13.36     0.0   122.1    11.3   \n",
       "2017-11-01 01:00:00   12.58    11.4   11.86   13.04     0.0   110.3    10.9   \n",
       "2017-11-01 01:30:00   12.50    10.8   11.53   11.83     0.0    83.6    11.2   \n",
       "2017-11-01 02:00:00   12.98    12.0   11.57   12.25     0.0    91.4    10.9   \n",
       "\n",
       "                     Site_8  Site_9  Site_10  ...  Site_39  Site_40  Site_41  \\\n",
       "2017-11-01 00:00:00    21.8     9.7     11.9  ...    22.20      0.0      0.0   \n",
       "2017-11-01 00:30:00    17.7     9.0     12.4  ...    21.68      0.0      0.0   \n",
       "2017-11-01 01:00:00    17.5     9.1     12.0  ...    21.56      0.0      0.0   \n",
       "2017-11-01 01:30:00    16.5    12.4     11.5  ...    21.28      0.0      0.0   \n",
       "2017-11-01 02:00:00    16.3    12.4     11.3  ...    40.48      0.0      0.0   \n",
       "\n",
       "                     Site_42  Site_43  Site_44  Site_45  Site_47  Site_46  \\\n",
       "2017-11-01 00:00:00        0    23.30    36.74    48.48    68.70    32.13   \n",
       "2017-11-01 00:30:00        0    22.14    36.44    50.18    69.09    29.02   \n",
       "2017-11-01 01:00:00        0    21.88    35.93    49.44    67.52    26.65   \n",
       "2017-11-01 01:30:00        0    22.69    45.25    49.57    68.48    25.28   \n",
       "2017-11-01 02:00:00        0    22.75    62.94    48.58    75.30    23.65   \n",
       "\n",
       "                     Site_48  \n",
       "2017-11-01 00:00:00    19.39  \n",
       "2017-11-01 00:30:00    19.54  \n",
       "2017-11-01 01:00:00    20.10  \n",
       "2017-11-01 01:30:00    19.46  \n",
       "2017-11-01 02:00:00    19.18  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_data_path = f\"s3://{data_bucket}/{subfolder}/data\"\n",
    "s3_output_path = f\"s3://{data_bucket}/{subfolder}/output\"\n",
    "df = pd.read_csv(f's3://{data_bucket}/{subfolder}/{dataset}', index_col=0)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erläuterung oben: Für 13 Monate wurde alle 30 Minuten der aktuelle Energiebedarf für alle 48 Standorte (Spalten) ermittelt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in dataset: 19632\n",
      "Number of columns in dataset: 48\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of rows in dataset: {df.shape[0]}')\n",
    "print(f'Number of columns in dataset: {df.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 : Get the data in the right shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zwar bieten uns die Datenpunkte alle 30 Minuten eine sehr gute Basis um etwaige Peaks zu identifizieren, jedoch ist dieses Intervall für unsere Aufgabe zu feingranular. Für die weitere Verarbeitung der Daten wollen wir daher den Energiebedarf auf die Einheit \"Tag\" summieren.\n",
    "Dafür wird zunächst der Index des Dataframes in ein datetime-Format umgewandelt. Anschließend sind wir in der Lage durch die resample-Funktion anhand des Tages ('D‘) zu summieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.index = pd.to_datetime(df.index)\n",
    "daily_df = df.resample('D').sum()\n",
    "daily_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(daily_df.shape)\n",
    "print(f'Time series starts at {daily_df.index[0]} and ends at {daily_df.index[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frage 1\n",
    "\n",
    "**Wie sind die beiden Werte (409, 48) zu interpretieren?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handling fehlender Werte**\n",
    "\n",
    "fillna() ersetzt fehlende Werte mit dem vorherigen Wert. \n",
    "Da wir jedoch wissen, dass der Wert der Vorwoche ggb dem Vortag eine bessere Aussagekraft hat, setzen wir shift(7). So ersetzten wir ggf. einen fehlenden Montag-Wert mit dem Montag-Wert der Vorwoche, statt mit dem Sonntag-Wert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_df = daily_df.fillna(daily_df.shift(7))\n",
    "daily_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anschließend wollen wir uns die Time-Series-Daten wieder visualisieren, um ein Gefühl für die Daten zu bekommen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of time series:',daily_df.shape[1])\n",
    "fig, axs = plt.subplots(5, 2, figsize=(20, 20), sharex=True)\n",
    "axx = axs.ravel()\n",
    "indices = [0,1,2,3,4,5,40,41,42,43]\n",
    "for i in indices:\n",
    "    plot_num = indices.index(i)\n",
    "    daily_df[daily_df.columns[i]].loc[\"2017-11-01\":\"2018-01-31\"].plot(ax=axx[plot_num])\n",
    "    axx[plot_num].set_xlabel(\"date\")    \n",
    "    axx[plot_num].set_ylabel(\"kW consumption\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optisch gibt es einige auffällige Korrelationen, die DeepAR wahrscheinlich erkennen und nutzen wird!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Erstellen des Trainings- und Test-Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepAR erwartet den Input (also die Zeitreihen) in JSON-Dateien. Um die JSON-Datei zu erstellen, müssen wir die Daten ein wenig bearbeiten:\n",
    ">1. Konvertierung der Daten von einem DataFrame in eine Liste von Reihen\n",
    ">2. Zurückhalten von 30 Tagen aus dem Trainingsdatensatz, damit das Modell nicht mit Daten trainiert wird, die es zum Testen verwenden soll\n",
    ">3. Erstellen der JSON-Dateien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_power_consumption_per_site = []\n",
    "for column in daily_df.columns:\n",
    "    site_consumption = np.trim_zeros(daily_df[column], trim='f')\n",
    "    site_consumption = site_consumption.fillna(0)\n",
    "    daily_power_consumption_per_site.append(site_consumption)\n",
    "    \n",
    "print(f'Time series covers {len(daily_power_consumption_per_site[0])} days.')\n",
    "print(f'Time series starts at {daily_power_consumption_per_site[0].index[0]}')\n",
    "print(f'Time series ends at {daily_power_consumption_per_site[0].index[-1]}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir legen fest, welche Tage für das Trainings- bzw. für das Testdatenset als Begrezung gelten sollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = 'D' #frequenz: Tage (days)\n",
    "prediction_length = 30 #Tage die unser Modell vorhersagen soll, \n",
    "#diese müssen wir als test-datenset von der Zeitreihe \"abschneiden\"\n",
    "#anmerkung: die fehlenden 15 tage sind dezember\n",
    "\n",
    "start_date = pd.Timestamp(\"2017-11-01 00:00:00\", freq=freq)\n",
    "end_training = start_date + 364 * start_date.freq\n",
    "end_testing = end_training + prediction_length * start_date.freq\n",
    "\n",
    "print(f'End training: {end_training}, End testing: {end_testing}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun erstellen wir eine python-Liste. Diese wird im nächsten Schritt in JSON-Dateien gespeichert, die DeepAR verarbeiten kann. Dabei werden zwei Variablen übergeben \"start\" und \"target\". Also legen wir für \"training\" und \"test\" jeweils eine Liste an, die anschließend in ein json-File überführt und im s3-Bucket abgelegt wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [\n",
    "    {\n",
    "        \"start\": str(start_date),\n",
    "        \"target\": ts[start_date:end_training].tolist()\n",
    "    }\n",
    "    for ts in daily_power_consumption_per_site\n",
    "]\n",
    "\n",
    "test_data = [\n",
    "    {\n",
    "        \"start\": str(start_date),\n",
    "        \"target\": ts[start_date:end_testing].tolist()\n",
    "    }\n",
    "    for ts in daily_power_consumption_per_site\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dicts_to_s3(path, data):\n",
    "    with s3.open(path, 'wb') as f:\n",
    "        for d in data:\n",
    "            f.write(json.dumps(d).encode(\"utf-8\"))\n",
    "            f.write(\"\\n\".encode('utf-8'))\n",
    "            \n",
    "write_dicts_to_s3(f'{s3_data_path}/train/train.json', training_data)\n",
    "write_dicts_to_s3(f'{s3_data_path}/test/test.json', test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anschließend wird wieder das Modell trainiert. Wir wählen das \"forecasting-deepar\"-model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_path = f's3://{data_bucket}/{subfolder}/output'\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "from sagemaker import image_uris \n",
    "image_name = sagemaker.image_uris.retrieve(\"forecasting-deepar\", sess.boto_region_name, \"latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir richten den Estimator ein und übergeben die sagemaker-Einstellungen für das Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sess,\n",
    "    image_uri=image_name,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m4.4xlarge',\n",
    "    base_job_name='ch6-energy-usage',\n",
    "    output_path=s3_output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparamtertuning\n",
    "\n",
    "Für das Hyperparametertuning stehen für uns in diesem Modell insbesondere context_length und prediction_length im Fokus:\n",
    "\n",
    "**context_length**\n",
    "The number of time-points that the model gets to see before making the prediction.\n",
    "\n",
    "**prediction_length**\n",
    "The number of time-steps that the model is trained to predict, also called the forecast horizon. The trained model always generates forecasts with this length. It can't generate longer forecasts. The prediction_length is fixed when a model is trained and it cannot be changed later.\n",
    "\n",
    "**Weitere Hyperparamter**\n",
    "Link: https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_hyperparameters.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(\n",
    "    time_freq=freq,\n",
    "    epochs=\"400\",\n",
    "    early_stopping_patience=\"40\",\n",
    "    mini_batch_size=\"64\",\n",
    "    learning_rate=\"5E-4\",\n",
    "    context_length=\"90\",\n",
    "    prediction_length=str(prediction_length)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun findet das tatsächliche Training statt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_channels = {\n",
    "    \"train\": \"{}/train/\".format(s3_data_path),\n",
    "    \"test\": \"{}/test/\".format(s3_data_path)\n",
    "}\n",
    "estimator.fit(inputs=data_channels, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Host the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = 'energy-usage'\n",
    "\n",
    "try:\n",
    "    sess.delete_endpoint(endpoint_name)\n",
    "    sess.delete_endpoint_config(endpoint_name)\n",
    "    print('Warning: Existing endpoint and configuration deleted to make way for your new endpoint.')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import IdentitySerializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der folgenden Zelle wird die DeepARPredictor-Klasse definiert, die im folgenden nutzen wollen. Diesen sehr technischen Part müssen wir nur ausführen, aber nicht detailliert nachvollziehen.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepARPredictor(sagemaker.predictor.Predictor):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(\n",
    "            *args,\n",
    "            # serializer=JSONSerializer(),\n",
    "            serializer=IdentitySerializer(content_type=\"application/json\"),\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        ts,\n",
    "        cat=None,\n",
    "        dynamic_feat=None,\n",
    "        num_samples=100,\n",
    "        return_samples=False,\n",
    "        quantiles=[\"0.1\", \"0.5\", \"0.9\"],\n",
    "    ):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "        corresponding category listed in `cat`.\n",
    "\n",
    "        ts -- `pandas.Series` object, the time series to predict\n",
    "        cat -- integer, the group associated to the time series (default: None)\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "        return_samples -- boolean indicating whether to include samples in the response (default: False)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: [\"0.1\", \"0.5\", \"0.9\"])\n",
    "\n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        prediction_time = ts.index[-1] + ts.index.freq\n",
    "        quantiles = [str(q) for q in quantiles]\n",
    "        req = self.__encode_request(ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, ts.index.freq, prediction_time, return_samples)\n",
    "\n",
    "    def __encode_request(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles):\n",
    "        instance = series_to_dict(\n",
    "            ts, cat if cat is not None else None, dynamic_feat if dynamic_feat else None\n",
    "        )\n",
    "\n",
    "        configuration = {\n",
    "            \"num_samples\": num_samples,\n",
    "            \"output_types\": [\"quantiles\", \"samples\"] if return_samples else [\"quantiles\"],\n",
    "            \"quantiles\": quantiles,\n",
    "        }\n",
    "\n",
    "        http_request_data = {\"instances\": [instance], \"configuration\": configuration}\n",
    "\n",
    "        return json.dumps(http_request_data).encode(\"utf-8\")\n",
    "\n",
    "    def __decode_response(self, response, freq, prediction_time, return_samples):\n",
    "        # we only sent one time series so we only receive one in return\n",
    "        # however, if possible one will pass multiple time series as predictions will then be faster\n",
    "        predictions = json.loads(response.decode(\"utf-8\"))[\"predictions\"][0]\n",
    "        prediction_length = len(next(iter(predictions[\"quantiles\"].values())))\n",
    "        prediction_index = pd.date_range(\n",
    "            start=prediction_time, freq=freq, periods=prediction_length\n",
    "        )\n",
    "        if return_samples:\n",
    "            dict_of_samples = {\"sample_\" + str(i): s for i, s in enumerate(predictions[\"samples\"])}\n",
    "        else:\n",
    "            dict_of_samples = {}\n",
    "        return pd.DataFrame(\n",
    "            data={**predictions[\"quantiles\"], **dict_of_samples}, index=prediction_index\n",
    "        )\n",
    "\n",
    "    def set_frequency(self, freq):\n",
    "        self.freq = freq\n",
    "\n",
    "\n",
    "def encode_target(ts):\n",
    "    return [x if np.isfinite(x) else \"NaN\" for x in ts]\n",
    "\n",
    "\n",
    "def series_to_dict(ts, cat=None, dynamic_feat=None):\n",
    "    \"\"\"Given a pandas.Series object, returns a dictionary encoding the time series.\n",
    "\n",
    "    ts -- a pands.Series object with the target time series\n",
    "    cat -- an integer indicating the time series category\n",
    "\n",
    "    Return value: a dictionary\n",
    "    \"\"\"\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": encode_target(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    if dynamic_feat is not None:\n",
    "        obj[\"dynamic_feat\"] = dynamic_feat\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt können wir das Modell bereitstellen und einen Endpunkt erstellen, der mit unserer benutzerdefinierten DeepARPredictor-Klasse abgefragt werden kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    predictor_cls=DeepARPredictor,\n",
    "    endpoint_name=endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Make Predictions and Plot Results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das 0.1-Quantil entspricht hier der unteren Grenze eines 80%-Konfidenzintervalls während das 0.9-Quantil der oberen Grenze eines 80%-Konfidenzintervalls entspricht. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictor.predict(\n",
    "    ts=daily_power_consumption_per_site[0][start_date+30*start_date.freq:end_training],\n",
    "    quantiles=[0.1, 0.5, 0.9]\n",
    "    ).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun gilt es noch die Funktion anzulegen, die uns die Vorhersage plottet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(\n",
    "    predictor, \n",
    "    target_ts,\n",
    "    end_training=end_training, \n",
    "    plot_weeks=12,\n",
    "    confidence=80\n",
    "):\n",
    "    frq = end_training.freq\n",
    "    print(f\"Calling served model to generate predictions from {end_training} to {end_training+prediction_length*frq}\")\n",
    "    low_quantile = 0.5 - confidence * 0.005\n",
    "    up_quantile = confidence * 0.005 + 0.5\n",
    "        \n",
    "    plot_history = plot_weeks * 7\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 3))\n",
    "    ax = plt.subplot(1,1,1)\n",
    "    \n",
    "    #vorhersagen\n",
    "    prediction = predictor.predict(ts=target_ts[:end_training], quantiles=[low_quantile, 0.5, up_quantile])\n",
    "    \n",
    "    #tatsächliche Testset-Werte\n",
    "    target_section = target_ts[end_training-plot_history*frq:end_training+prediction_length*frq]\n",
    "    target_section.plot(color=\"black\", label='Actual')\n",
    "    \n",
    "    ax.fill_between(\n",
    "        prediction[str(low_quantile)].index, \n",
    "        prediction[str(low_quantile)].values, \n",
    "        prediction[str(up_quantile)].values, \n",
    "        color=\"b\", alpha=0.3, label=f'{confidence}% confidence interval'\n",
    "    )\n",
    "    \n",
    "    prediction[\"0.5\"].plot(color=\"red\", label='P50')\n",
    "    #P50 (0.5) - The true value is expected to be lower than the predicted value 50% of the time. \n",
    "    #This is also known as the median forecast\n",
    "    \n",
    "    ax.legend(loc=2)    \n",
    "    \n",
    "    ax.set_ylim(target_section.min() * 0.5, target_section.max() * 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotten der Vorhersagen\n",
    "\n",
    "Die Diagramme zeigen den vorhergesagten (rot) gegenüber dem tatsächlichen (schwarz) Energieverbrauch für die Standorte (*sites = [...]*) im November 2018. Auffällig dabei ist, dass die Diagramme durchaus unterschiedliche Ausprägungen haben. So ist sites[0] durch eher wenige Ausschläge geprägt, während sites[33] deutlich einen wöchentlichen Verlauf erkennen lässt. Trotzdem gibt das Modell zuverlässige Vorhersagen zu beiden Standorten.\n",
    "\n",
    "*(Das Konfidenzintervall gibt den Bereich an, der mit einer gewissen Wahrscheinlichkeit den Parameter einer Verteilung einer Zufallsvariablen einschließt.)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = [0,1,26,33,39,42,47]\n",
    "for i in sites:\n",
    "    plot_num = sites.index(i)\n",
    "    plot(\n",
    "        predictor,\n",
    "        target_ts=daily_power_consumption_per_site[i][start_date+30*start_date.freq:],\n",
    "        plot_weeks=2,\n",
    "        confidence=80\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frage 2\n",
    "\n",
    "**Wie sieht die Vorhersage für die Standorte 6,7,8 mit einem Vorlauf von 12 Wochen aus, wenn wir ein Konfidenzintervall von 95% darstellen wollen?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wie sind diese Ausgaben zu interpretieren?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Berechnung von objektiven Statistiken zur Genauigkeit unseres Modells mit dem Test-Datenset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MAPE** misst den \"Mean Absolute Percentage Error\" (mittleren absoluten prozentualen Fehler). Der Hauptgrund für die Verwendung von MAPE gegenüber dem RMSE (\"Root Mean Squared Error\" aus dem Modelltraining) besteht darin, dass er die Fehler in Prozenten und nicht in absoluten Werten bewertet. Eine Vorhersage von 11 für einen Wert von 10 wird also genauso behandelt gleich behandelt wie eine Vorhersage von 90 für einen Wert von 100.\n",
    "\n",
    "Link: https://docs.aws.amazon.com/forecast/latest/dg/metrics.html#metrics-mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sammeln von 30-Tage-Vorhersagen über alle Zeitreihen\n",
    "predictions= []\n",
    "for i, ts in enumerate(daily_power_consumption_per_site):\n",
    "    # Aufruf der prediction\n",
    "    predictions.append(predictor.predict(ts=ts[start_date+30*start_date.freq:end_training])['0.5'].sum())\n",
    "\n",
    "usages = [ts[end_training+1*start_date.freq:end_training+30*start_date.freq].sum() \\\n",
    "          for ts in daily_power_consumption_per_site]\n",
    "\n",
    "for p,u in zip(predictions,usages):\n",
    "    print(f'Predicted {p} kwh but usage was {u} kwh,')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'MAPE: {round(mape(usages, predictions),1)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frage 3\n",
    "\n",
    "**Warum ist für diesen Anwendungsfall der MAPE eher geeignet als der RMSE?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the Endpoint (recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment out this cell to remove the endpoint if you want the endpoint to exist after \"run all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
